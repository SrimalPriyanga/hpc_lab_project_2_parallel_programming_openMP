\documentclass[unicode,11pt,a4paper,oneside,numbers=endperiod,openany]{scrartcl}

\input{assignment.sty}
% Provide improved math macros (\text, etc.) used in the report
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{geometry}
\geometry{margin=1in}
\lstset{
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red},
  showstringspaces=false,
  breaklines=true,
  frame=single,
  captionpos=b,
  language=C
}
\begin{document}


\setassignment

\serieheader{High-Performance Computing Lab}{Institute of Computing}{Student: FULL NAME}{Discussed with: FULL NAME}{Solution for Project 2}{}
\newline

\assignmentpolicy
This project will introduce you to parallel programming using OpenMP. 

\section{Parallel reduction operations using OpenMP \punkte{20}}

\subsection{Dot Product}
\subsection*{1 Parallel Implementations}

Two parallel implementations of the dot product were developed using OpenMP.

\begin{itemize}
    \item \textbf{Reduction Clause:} This method uses \texttt{\#pragma omp parallel for reduction(+:alpha)} to safely accumulate partial results from each thread. It is efficient and minimizes synchronization overhead.
    
    \item \textbf{Critical Directive:} This method uses \texttt{\#pragma omp critical} to serialize access to the shared accumulation variable, ensuring correctness but introducing significant performance penalties due to thread contention.
\end{itemize}

Correctness of both implementations was verified against the serial baseline.

\subsection*{2 Strong Scaling Analysis}

Strong scaling tests were conducted on the Rosa cluster for both parallel versions using thread counts $t = 1, 2, 4, 8, 16, 20$ and vector sizes $N = 10^5, 10^6, 10^7, 10^8, 10^9$.

\subsection*{Reduction Method}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{./figures/Figure 1: Strong Scaling - Reduction Method.png}
    \caption{Strong scaling performance using the reduction method.}
\end{figure}

The reduction method demonstrates consistent improvement in execution time as the number of threads increases. For larger vector sizes, scaling is nearly ideal up to 8 threads, with diminishing returns beyond that point due to hardware limitations and overhead.

\subsection*{Critical Method}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{./figures/Figure 2: Strong Scaling - Critical Method.png}
    \caption{Strong scaling performance using the critical method.}
\end{figure}

The critical method exhibits poor scalability. Execution time increases with additional threads due to contention at the critical section, particularly for small vector sizes. For larger $N$, performance stabilizes but remains significantly inferior to the reduction method.

\newpage
\subsection*{3 Parallel Efficiency Analysis}

Parallel efficiency was computed as:
\[
\text{Efficiency} = \frac{\text{Speedup}}{\text{Number of Threads}}, \quad \text{Speedup} = \frac{\text{Serial Time}}{\text{Parallel Time}}
\]

\subsection*{Reduction Method}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{./figures/Figure 3: Parallel Efficiency - Reduction Method.png}
    \caption{Parallel efficiency using the reduction method.}
\end{figure}

Efficiency remains high for small thread counts and large vector sizes. For $N \geq 10^7$, efficiency exceeds 60\% even at 20 threads, indicating effective utilization of parallel resources.
\subsection*{Critical Method}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{./figures/Figure 4: Parallel Efficiency - Critical Method.png}
    \caption{Parallel efficiency using the critical method.}
\end{figure}

Efficiency declines sharply with increasing thread counts, particularly for small $N$. Even for $N = 10^9$, efficiency remains low due to synchronization overhead. This behavior is consistent across all vector sizes, confirming that the synchronization cost outweighs the benefits of parallelism.

\newpage
\section*{Discussion}

\subsection*{OpenMP Overhead}

The critical directive introduces significant overhead due to serialized access to the shared variable. In contrast, the reduction clause avoids this issue by efficiently combining partial results.

\subsection*{Thread Count vs. Workload}

For small vector sizes ($N \leq 10^6$), parallelization offers limited benefit, as overhead outweighs performance gains. For larger sizes ($N \geq 10^7$), multi-threading becomes advantageous, particularly when using the reduction method.

\subsection*{Conclusion}

The reduction method is preferred for parallel dot product computation. It demonstrates good scalability and maintains high efficiency for large workloads. The critical method should be avoided in performance-sensitive applications.

\subsection*{1.2 Approximating $\pi$}

The value of $\pi$ was approximated using the midpoint rule applied to the integral:

\[
\pi = \int_0^1 \frac{4}{1 + x^2} \, dx
\]

A fixed number of subintervals, $N = 10^{10}$, was used to ensure a computationally intensive workload suitable for parallel analysis. A serial version was implemented using a standard loop, followed by a parallel version using OpenMP with the \texttt{\#pragma omp parallel for reduction(+:sum)} directive. This approach was selected for its simplicity and efficiency, as it avoids locking overhead and ensures safe accumulation of partial sums across threads.

Execution times were recorded for both serial and parallel versions across thread counts $t = 1, 2, 4, 8$. Table~\ref{tab:pi_scaling} summarizes the results, including calculated speedup and parallel efficiency:

\begin{table}[h!]
\centering
\caption{Performance results for $\pi$ approximation with $N = 10^{10}$}
\label{tab:pi_scaling}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Threads} & \textbf{Serial Time (s)} & \textbf{Parallel Time (s)} & \textbf{Speedup} & \textbf{Efficiency} \\
\hline
1 & 53.93037 & 53.93042 & 1.0000 & 1.0000 \\
2 & 53.93950 & 26.97084 & 1.9999 & 1.0000 \\
4 & 53.93071 & 13.48577 & 3.9991 & 0.9998 \\
8 & 53.93037 & 6.743196 & 7.9977 & 0.9997 \\
\hline
\end{tabular}
\end{table}

\subsection*{Speedup and Efficiency Analysis}

Speedup quantifies how much faster the parallel version is compared to the serial one:

\[
\text{Speedup} = \frac{\text{Serial Time}}{\text{Parallel Time}} = \frac{53.93037}{6.743196} \approx 7.9977
\]

Using 8 threads made the program almost 8 times faster. Parallel efficiency measures how well the threads are utilized:

\[
\text{Efficiency} = \frac{\text{Speedup}}{\text{Number of Threads}} = \frac{7.9977}{8} \approx 0.9997
\]

This corresponds to approximately 99.97\% efficiency, which is excellent and indicates minimal overhead.

\subsection*{Discussion}

The results demonstrate near-perfect linear scaling up to 8 threads, confirming strong scaling behavior where the problem size remains constant while the number of processing units increases. The OpenMP \texttt{reduction} clause provided an efficient and scalable solution for parallelization. The task is highly parallelizable and benefits significantly from multi-threading.

Figure~\ref{fig:pi_scaling_plots} illustrates the speedup and efficiency trends on a log-scaled axis:

\begin{figure}[h!]
\centering
\includegraphics[width=0.9\textwidth]{./figures/Figure 5: updated_pi_scaling_plots.png}
\caption{Speedup and Parallel Efficiency for $\pi$ Approximation}
\label{fig:pi_scaling_plots}
\end{figure}


\newpage
\section{The Mandelbrot set using OpenMP \punkte{20}}

\subsection{Sequential Implementation}
The Mandelbrot set was computed using the provided skeleton code. Each pixel corresponds to a complex number $c$, and the iterative function:
\[
z_{n+1} = z_n^2 + c, \quad z_0 = 0
\]
determines whether the sequence remains bounded within a radius of 2. The number of iterations before divergence defines the pixel color. The image was generated using \texttt{pngwriter}, and performance metrics were recorded using the recommended format.

\subsection{Benchmarking (Sequential)}
Performance was evaluated for multiple image sizes. Table~\ref{tab:seq_perf} summarizes the results.

\begin{table}[h!]
\centering
\caption{Sequential Performance Across Image Sizes}
\label{tab:seq_perf}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Image Size} & \textbf{Total Time (s)} & \textbf{Iterations/sec} & \textbf{MFlop/s} \\
\hline
512$\times$512    & 2.43   & $2.08\times10^8$ & 1660 \\
1024$\times$1024  & 9.74   & $2.08\times10^8$ & 1660 \\
2048$\times$2048  & 38.95  & $2.08\times10^8$ & 1660 \\
4096$\times$4096  & 155.82 & $2.08\times10^8$ & 1660 \\
8192$\times$8192  & 623.25 & $2.08\times10^8$ & 1660 \\
\hline
\end{tabular}
\end{table}

\textbf{Observation:} Iterations/sec and MFlop/s remain constant, confirming linear scaling with image size.

\subsection{Parallel Implementation}
The outer loop over image rows was parallelized using OpenMP:
\begin{verbatim}
#pragma omp parallel for private(i, cx, cy, x, y, x2, y2, n) reduction(+:nTotalIterationsCount)
\end{verbatim}

Variables were privatized to prevent race conditions, and \texttt{nTotalIterationsCount} was updated using a reduction clause. Compilation used:
\begin{verbatim}
gcc -fopenmp mandel_parallel.c -o mandel_parallel -lpng
\end{verbatim}

Execution was automated for thread counts: 1, 2, 4, 8, and 16.

\subsection{Benchmarking (Parallel)}
Performance for an image size of 4096$\times$4096 pixels is shown in Table~\ref{tab:parallel_perf}.

\begin{table}[h!]
\centering
\caption{Parallel Performance and Scaling}
\label{tab:parallel_perf}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Threads} & \textbf{Total Time (s)} & \textbf{Speedup} & \textbf{Efficiency} \\
\hline
1  & 155.82 & 1.00 & 1.00 \\
2  & 78.09  & 2.00 & 1.00 \\
4  & 75.34  & 2.07 & 0.52 \\
8  & 50.60  & 3.08 & 0.38 \\
16 & 28.61  & 5.45 & 0.34 \\
\hline
\end{tabular}
\end{table}

\subsection{Calculation Formulas}
Speedup ($S$) and efficiency ($E$) were computed as:
\[
S = \frac{T_1}{T_p}, \quad E = \frac{S}{p}
\]
where $T_1$ is the execution time with one thread, $T_p$ is the execution time with $p$ threads.

\subsection{Strong Scaling Plot}
\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{./figures/Figure 6: strong_scaling_plot.png}
\caption{Strong Scaling of Mandelbrot Set using OpenMP}
\label{fig:strong_scaling}
\end{figure}

\subsection{Rendered Mandelbrot Image}
The final parallel implementation produced the Mandelbrot set image shown in Figure~\ref{fig:mandel_image}. This image was generated using 8 threads for an image size of 4096$\times$4096 pixels.

\begin{figure}[h!]
\centering
\includegraphics[width=0.7\textwidth]{./figures/Figure 7: mandel_4096x4096_1threads.png}
\caption{Mandelbrot Set rendered using OpenMP with 8 threads (4096$\times$4096 pixels)}
\label{fig:mandel_image}
\end{figure}

\subsection{Discussion}
\begin{itemize}
\item Speedup is nearly ideal up to 2 threads.
\item Efficiency declines beyond 4 threads due to memory bandwidth and synchronization overhead.
\item At 16 threads, runtime decreases by $\sim$81\%, demonstrating parallelization benefits for large workloads despite reduced efficiency.
\end{itemize}

\subsection{Conclusion}
OpenMP parallelization significantly accelerates Mandelbrot computation for large images. Strong scaling is limited by hardware constraints, but parallel execution remains advantageous for high-resolution rendering.


\newpage
\section{Bug hunt \punkte{15}}

This section documents the identification and correction of five OpenMP bugs.

\subsection{Bug 1: \texttt{omp\_bug1.c}}

\textbf{Problem:} Incorrect use of \texttt{\#pragma omp parallel for} followed by a block.

\begin{lstlisting}[caption=Buggy Code]
#pragma omp parallel for shared(a, b, c, chunk) private(i, tid) \
schedule(static, chunk)
{
    tid = omp_get_thread_num();
    for (i = 0; i < N; i++) {
        c[i] = a[i] + b[i];
        printf("tid= %d i= %d c[i]= %f\n", tid, i, c[i]);
    }
}
\end{lstlisting}

\textbf{Fix:} Move the loop directly after the pragma.

\begin{lstlisting}[caption=Corrected Code]
#pragma omp parallel for shared(a, b, c, chunk) private(i, tid) schedule(static, chunk)
for (i = 0; i < N; i++) {
    tid = omp_get_thread_num();
    c[i] = a[i] + b[i];
    printf("tid= %d i= %d c[i]= %f\n", tid, i, c[i]);
}
\end{lstlisting}

\subsection{Bug 2: \texttt{omp\_bug2.c}}

\textbf{Problem:} Shared variable \texttt{total} causes race conditions.

\begin{lstlisting}[caption=Buggy Code]
float total = 0.0;
#pragma omp parallel
{
    #pragma omp for schedule(dynamic, 10)
    for (i = 0; i < 1000000; i++)
        total += i * 1.0;
}
\end{lstlisting}

\textbf{Fix:} Use OpenMP reduction clause.

\begin{lstlisting}[caption=Corrected Code]
float total = 0.0;
#pragma omp parallel for reduction(+:total) schedule(dynamic, 10)
for (i = 0; i < 1000000; i++) {
    total += i * 1.0;
}
\end{lstlisting}

\subsection{Bug 3: \texttt{omp\_bug3.c}}

\textbf{Problem:} Large array \texttt{c[N]} declared private causes stack overflow.

\begin{lstlisting}[caption=Buggy Code]
#pragma omp parallel private(c, i, tid, section)
\end{lstlisting}

\textbf{Fix:} Declare \texttt{c} as shared.

\begin{lstlisting}[caption=Corrected Code]
#pragma omp parallel shared(c) private(i, tid, section)
\end{lstlisting}

\subsection{Bug 4: \texttt{omp\_bug4.c}}

\textbf{Problem:} Large 2D array \texttt{a[N][N]} declared private causes segmentation fault.

\begin{lstlisting}[caption=Buggy Code]
#pragma omp parallel shared(nthreads) private(i, j, tid, a)
\end{lstlisting}

\textbf{Fix:} Make \texttt{a} shared.

\begin{lstlisting}[caption=Corrected Code]
#pragma omp parallel shared(nthreads, a) private(i, j, tid)
\end{lstlisting}

\subsection{Bug 5: \texttt{omp\_bug5.c}}

\textbf{Problem:} Deadlock due to inconsistent locking order.

\begin{lstlisting}[caption=Buggy Code]
omp_set_lock(&locka);
// ...
omp_set_lock(&lockb);
\end{lstlisting}

\textbf{Fix:} Ensure consistent locking order in both threads.

\begin{lstlisting}[caption=Corrected Code]
omp_set_lock(&locka);
omp_set_lock(&lockb);
// ...
omp_unset_lock(&lockb);
omp_unset_lock(&locka);
\end{lstlisting}

\newpage
\section{Parallel histogram calculation using OpenMP \punkte{15}}


\newpage
\section{Parallel loop dependencies with OpenMP \punkte{15}}


\newpage
\section{Quality of the Report \punkte{15}}


\end{document}