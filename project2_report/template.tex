\documentclass[unicode,11pt,a4paper,oneside,numbers=endperiod,openany]{scrartcl}

\input{assignment.sty}
% Provide improved math macros (\text, etc.) used in the report
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{geometry}
\geometry{margin=1in}
\lstset{
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red},
  showstringspaces=false,
  breaklines=true,
  frame=single,
  captionpos=b,
  language=C
}
\begin{document}


\setassignment

\serieheader{High-Performance Computing Lab}{Institute of Computing}{Student: Srimal Fonseka}{Discussed with: FULL NAME}{Solution for Project 2}{}
\newline

\assignmentpolicy
This project will introduce you to parallel programming using OpenMP. 

\section{Parallel reduction operations using OpenMP \punkte{20}}

\subsection{Dot Product}
\subsection*{1 Parallel Implementations}

Two parallel implementations of the dot product were developed using OpenMP.

\begin{itemize}
    \item \textbf{Reduction Clause:} This method uses \texttt{\#pragma omp parallel for reduction(+:alpha)} to safely accumulate partial results from each thread. It is efficient and minimizes synchronization overhead.
    
    \item \textbf{Critical Directive:} This method uses \texttt{\#pragma omp critical} to serialize access to the shared accumulation variable, ensuring correctness but introducing significant performance penalties due to thread contention.
\end{itemize}

Correctness of both implementations was verified against the serial baseline.

\subsection*{2 Strong Scaling Analysis}

Strong scaling tests were conducted on the Rosa cluster for both parallel versions using thread counts $t = 1, 2, 4, 8, 16, 20$ and vector sizes $N = 10^5, 10^6, 10^7, 10^8, 10^9$.

\subsection*{Reduction Method}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{./figures/Figure 1: Strong Scaling - Reduction Method.png}
    \caption{Strong scaling performance using the reduction method.}
\end{figure}

The reduction method demonstrates consistent improvement in execution time as the number of threads increases. For larger vector sizes, scaling is nearly ideal up to 8 threads, with diminishing returns beyond that point due to hardware limitations and overhead.

\subsection*{Critical Method}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{./figures/Figure 2: Strong Scaling - Critical Method.png}
    \caption{Strong scaling performance using the critical method.}
\end{figure}

The critical method exhibits poor scalability. Execution time increases with additional threads due to contention at the critical section, particularly for small vector sizes. For larger $N$, performance stabilizes but remains significantly inferior to the reduction method.

\newpage
\subsection*{3 Parallel Efficiency Analysis}

Parallel efficiency was computed as:
\[
\text{Efficiency} = \frac{\text{Speedup}}{\text{Number of Threads}}, \quad \text{Speedup} = \frac{\text{Serial Time}}{\text{Parallel Time}}
\]

\subsection*{Reduction Method}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{./figures/Figure 3: Parallel Efficiency - Reduction Method.png}
    \caption{Parallel efficiency using the reduction method.}
\end{figure}

Efficiency remains high for small thread counts and large vector sizes. For $N \geq 10^7$, efficiency exceeds 60\% even at 20 threads, indicating effective utilization of parallel resources.
\subsection*{Critical Method}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{./figures/Figure 4: Parallel Efficiency - Critical Method.png}
    \caption{Parallel efficiency using the critical method.}
\end{figure}

Efficiency declines sharply with increasing thread counts, particularly for small $N$. Even for $N = 10^9$, efficiency remains low due to synchronization overhead. This behavior is consistent across all vector sizes, confirming that the synchronization cost outweighs the benefits of parallelism.

\newpage
\section*{Discussion}

\subsection*{OpenMP Overhead}

The critical directive introduces significant overhead due to serialized access to the shared variable. In contrast, the reduction clause avoids this issue by efficiently combining partial results.

\subsection*{Thread Count vs. Workload}

For small vector sizes ($N \leq 10^6$), parallelization offers limited benefit, as overhead outweighs performance gains. For larger sizes ($N \geq 10^7$), multi-threading becomes advantageous, particularly when using the reduction method.

\subsection*{Conclusion}

The reduction method is preferred for parallel dot product computation. It demonstrates good scalability and maintains high efficiency for large workloads. The critical method should be avoided in performance-sensitive applications.

\subsection*{1.2 Approximating $\pi$}

The value of $\pi$ was approximated using the midpoint rule applied to the integral:

\[
\pi = \int_0^1 \frac{4}{1 + x^2} \, dx
\]

A fixed number of subintervals, $N = 10^{10}$, was used to ensure a computationally intensive workload suitable for parallel analysis. A serial version was implemented using a standard loop, followed by a parallel version using OpenMP with the \texttt{\#pragma omp parallel for reduction(+:sum)} directive. This approach was selected for its simplicity and efficiency, as it avoids locking overhead and ensures safe accumulation of partial sums across threads.

Execution times were recorded for both serial and parallel versions across thread counts $t = 1, 2, 4, 8$. Table~\ref{tab:pi_scaling} summarizes the results, including calculated speedup and parallel efficiency:

\begin{table}[h!]
\centering
\caption{Performance results for $\pi$ approximation with $N = 10^{10}$}
\label{tab:pi_scaling}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Threads} & \textbf{Serial Time (s)} & \textbf{Parallel Time (s)} & \textbf{Speedup} & \textbf{Efficiency} \\
\hline
1 & 53.93037 & 53.93042 & 1.0000 & 1.0000 \\
2 & 53.93950 & 26.97084 & 1.9999 & 1.0000 \\
4 & 53.93071 & 13.48577 & 3.9991 & 0.9998 \\
8 & 53.93037 & 6.743196 & 7.9977 & 0.9997 \\
\hline
\end{tabular}
\end{table}

\subsection*{Speedup and Efficiency Analysis}

Speedup quantifies how much faster the parallel version is compared to the serial one:

\[
\text{Speedup} = \frac{\text{Serial Time}}{\text{Parallel Time}} = \frac{53.93037}{6.743196} \approx 7.9977
\]

Using 8 threads made the program almost 8 times faster. Parallel efficiency measures how well the threads are utilized:

\[
\text{Efficiency} = \frac{\text{Speedup}}{\text{Number of Threads}} = \frac{7.9977}{8} \approx 0.9997
\]

This corresponds to approximately 99.97\% efficiency, which is excellent and indicates minimal overhead.

\subsection*{Discussion}

The results demonstrate near-perfect linear scaling up to 8 threads, confirming strong scaling behavior where the problem size remains constant while the number of processing units increases. The OpenMP \texttt{reduction} clause provided an efficient and scalable solution for parallelization. The task is highly parallelizable and benefits significantly from multi-threading.

Figure~\ref{fig:pi_scaling_plots} illustrates the speedup and efficiency trends on a log-scaled axis:

\begin{figure}[h!]
\centering
\includegraphics[width=0.9\textwidth]{./figures/Figure 5: updated_pi_scaling_plots.png}
\caption{Speedup and Parallel Efficiency for $\pi$ Approximation}
\label{fig:pi_scaling_plots}
\end{figure}


\newpage
\section{The Mandelbrot set using OpenMP \punkte{20}}

\subsection{Sequential Implementation}
The Mandelbrot set was computed using the provided skeleton code. Each pixel corresponds to a complex number $c$, and the iterative function:
\[
z_{n+1} = z_n^2 + c, \quad z_0 = 0
\]
determines whether the sequence remains bounded within a radius of 2. The number of iterations before divergence defines the pixel color. The image was generated using \texttt{pngwriter}, and performance metrics were recorded using the recommended format.

\subsection{Benchmarking (Sequential)}
Performance was evaluated for multiple image sizes. Table~\ref{tab:seq_perf} summarizes the results.

\begin{table}[h!]
\centering
\caption{Sequential Performance Across Image Sizes}
\label{tab:seq_perf}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Image Size} & \textbf{Total Time (s)} & \textbf{Iterations/sec} & \textbf{MFlop/s} \\
\hline
512$\times$512    & 2.43   & $2.08\times10^8$ & 1660 \\
1024$\times$1024  & 9.74   & $2.08\times10^8$ & 1660 \\
2048$\times$2048  & 38.95  & $2.08\times10^8$ & 1660 \\
4096$\times$4096  & 155.82 & $2.08\times10^8$ & 1660 \\
8192$\times$8192  & 623.25 & $2.08\times10^8$ & 1660 \\
\hline
\end{tabular}
\end{table}

\textbf{Observation:} Iterations/sec and MFlop/s remain constant, confirming linear scaling with image size.

\subsection{Parallel Implementation}
The outer loop over image rows was parallelized using OpenMP:
\begin{verbatim}
#pragma omp parallel for private(i, cx, cy, x, y, x2, y2, n) reduction(+:nTotalIterationsCount)
\end{verbatim}

Variables were privatized to prevent race conditions, and \texttt{nTotalIterationsCount} was updated using a reduction clause. Compilation used:
\begin{verbatim}
gcc -fopenmp mandel_parallel.c -o mandel_parallel -lpng
\end{verbatim}

Execution was automated for thread counts: 1, 2, 4, 8, and 16.

\subsection{Benchmarking (Parallel)}
Performance for an image size of 4096$\times$4096 pixels is shown in Table~\ref{tab:parallel_perf}.

\begin{table}[h!]
\centering
\caption{Parallel Performance and Scaling}
\label{tab:parallel_perf}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Threads} & \textbf{Total Time (s)} & \textbf{Speedup} & \textbf{Efficiency} \\
\hline
1  & 155.82 & 1.00 & 1.00 \\
2  & 78.09  & 2.00 & 1.00 \\
4  & 75.34  & 2.07 & 0.52 \\
8  & 50.60  & 3.08 & 0.38 \\
16 & 28.61  & 5.45 & 0.34 \\
\hline
\end{tabular}
\end{table}

\subsection{Calculation Formulas}
Speedup ($S$) and efficiency ($E$) were computed as:
\[
S = \frac{T_1}{T_p}, \quad E = \frac{S}{p}
\]
where $T_1$ is the execution time with one thread, $T_p$ is the execution time with $p$ threads.

\subsection{Strong Scaling Plot}
\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{./figures/Figure 6: strong_scaling_plot.png}
\caption{Strong Scaling of Mandelbrot Set using OpenMP}
\label{fig:strong_scaling}
\end{figure}

\subsection{Rendered Mandelbrot Image}
The final parallel implementation produced the Mandelbrot set image shown in Figure~\ref{fig:mandel_image}. This image was generated using 8 threads for an image size of 4096$\times$4096 pixels.

\begin{figure}[h!]
\centering
\includegraphics[width=0.7\textwidth]{./figures/Figure 7: mandel_4096x4096_1threads.png}
\caption{Mandelbrot Set rendered using OpenMP with 8 threads (4096$\times$4096 pixels)}
\label{fig:mandel_image}
\end{figure}

\subsection{Discussion}
\begin{itemize}
\item Speedup is nearly ideal up to 2 threads.
\item Efficiency declines beyond 4 threads due to memory bandwidth and synchronization overhead.
\item At 16 threads, runtime decreases by $\sim$81\%, demonstrating parallelization benefits for large workloads despite reduced efficiency.
\end{itemize}

\subsection{Conclusion}
OpenMP parallelization significantly accelerates Mandelbrot computation for large images. Strong scaling is limited by hardware constraints, but parallel execution remains advantageous for high-resolution rendering.


\newpage
\section{Bug hunt \punkte{15}}

This section documents the identification and correction of five OpenMP bugs.

\subsection{Bug 1: \texttt{omp\_bug1.c}}

\textbf{Problem:} Incorrect use of \texttt{\#pragma omp parallel for} followed by a block.

\begin{lstlisting}[caption=Buggy Code]
#pragma omp parallel for shared(a, b, c, chunk) private(i, tid) \
schedule(static, chunk)
{
    tid = omp_get_thread_num();
    for (i = 0; i < N; i++) {
        c[i] = a[i] + b[i];
        printf("tid= %d i= %d c[i]= %f\n", tid, i, c[i]);
    }
}
\end{lstlisting}

\textbf{Fix:} Move the loop directly after the pragma.

\begin{lstlisting}[caption=Corrected Code]
#pragma omp parallel for shared(a, b, c, chunk) private(i, tid) schedule(static, chunk)
for (i = 0; i < N; i++) {
    tid = omp_get_thread_num();
    c[i] = a[i] + b[i];
    printf("tid= %d i= %d c[i]= %f\n", tid, i, c[i]);
}
\end{lstlisting}

\subsection{Bug 2: \texttt{omp\_bug2.c}}

\textbf{Problem:} Shared variable \texttt{total} causes race conditions.

\begin{lstlisting}[caption=Buggy Code]
float total = 0.0;
#pragma omp parallel
{
    #pragma omp for schedule(dynamic, 10)
    for (i = 0; i < 1000000; i++)
        total += i * 1.0;
}
\end{lstlisting}

\textbf{Fix:} Use OpenMP reduction clause.

\begin{lstlisting}[caption=Corrected Code]
float total = 0.0;
#pragma omp parallel for reduction(+:total) schedule(dynamic, 10)
for (i = 0; i < 1000000; i++) {
    total += i * 1.0;
}
\end{lstlisting}

\subsection{Bug 3: \texttt{omp\_bug3.c}}

\textbf{Problem:} Large array \texttt{c[N]} declared private causes stack overflow.

\begin{lstlisting}[caption=Buggy Code]
#pragma omp parallel private(c, i, tid, section)
\end{lstlisting}

\textbf{Fix:} Declare \texttt{c} as shared.

\begin{lstlisting}[caption=Corrected Code]
#pragma omp parallel shared(c) private(i, tid, section)
\end{lstlisting}

\subsection{Bug 4: \texttt{omp\_bug4.c}}

\textbf{Problem:} Large 2D array \texttt{a[N][N]} declared private causes segmentation fault.

\begin{lstlisting}[caption=Buggy Code]
#pragma omp parallel shared(nthreads) private(i, j, tid, a)
\end{lstlisting}

\textbf{Fix:} Make \texttt{a} shared.

\begin{lstlisting}[caption=Corrected Code]
#pragma omp parallel shared(nthreads, a) private(i, j, tid)
\end{lstlisting}

\subsection{Bug 5: \texttt{omp\_bug5.c}}

\textbf{Problem:} Deadlock due to inconsistent locking order.

\begin{lstlisting}[caption=Buggy Code]
omp_set_lock(&locka);
// ...
omp_set_lock(&lockb);
\end{lstlisting}

\textbf{Fix:} Ensure consistent locking order in both threads.

\begin{lstlisting}[caption=Corrected Code]
omp_set_lock(&locka);
omp_set_lock(&lockb);
// ...
omp_unset_lock(&lockb);
omp_unset_lock(&locka);
\end{lstlisting}

\newpage
\section{Parallel histogram calculation using OpenMP \punkte{15}}

\subsection{Overview}
This section focuses on parallelizing a histogram computation over a large vector of integers using OpenMP. The vector contains $10^9$ elements, each in the range $[0, 15]$, and the histogram consists of 16 bins. The goal is to implement a thread-safe parallel version, benchmark its performance, and analyze strong scaling behavior.

\subsection{Implementation}
The serial version of the histogram is implemented in \texttt{hist\_seq.cpp}, where each element in the vector increments the corresponding bin in a shared histogram array. The parallel version in \texttt{hist\_omp.cpp} uses OpenMP with thread-local histograms to avoid race conditions and false sharing.

Each thread maintains its own local histogram, which is merged into the global histogram after the parallel region. This approach ensures correctness and improves performance by avoiding concurrent writes to shared memory.

\subsection{Benchmark Results}
The serial version executed in 0.83237 seconds. The parallel version was tested with increasing thread counts using the Rosa cluster. The results are shown in Table~\ref{tab:scaling} and Figure~\ref{fig:scaling}.

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Threads} & \textbf{Time (s)} & \textbf{Speedup} & \textbf{Efficiency (\%)} \\
\hline
1   & 0.8324 & 1.00  & 100.0 \\
2   & 0.8137 & 1.02  & 51.0  \\
4   & 0.3875 & 2.15  & 53.7  \\
8   & 0.2456 & 3.39  & 42.4  \\
16  & 0.2357 & 3.53  & 22.1  \\
32  & 0.1646 & 5.06  & 15.8  \\
64  & 0.1315 & 6.33  & 9.9   \\
\hline
\end{tabular}
\caption{Strong Scaling Results: Speedup and Parallel Efficiency of Histogram Computation}
\label{tab:scaling}
\end{table}

\begin{figure}[h!]
\centering
\includegraphics[width=0.7\textwidth]{./figures/Figure 8: strong_scaling_plot.png}
\caption{Strong Scaling Plot: Execution Time vs Number of Threads (log scale)}
\label{fig:scaling}
\end{figure}

\subsection{Performance Analysis}
The parallel implementation shows strong scaling up to 16–32 threads, with diminishing returns beyond that. Speedup improves significantly with thread count, but parallel efficiency drops due to increased overhead and memory contention.

False sharing was avoided by using thread-local histograms. Each thread writes to its own memory space, and results are merged after the parallel region. This strategy eliminates the need for synchronization primitives like \texttt{\#pragma omp critical}, improving both correctness and performance.

\subsection{Conclusion}
The OpenMP-parallelized histogram computation achieves substantial speedup and demonstrates effective strong scaling. The use of thread-local data structures ensures thread safety and avoids false sharing, making the implementation both efficient and scalable.


\newpage
\section{Parallel loop dependencies with OpenMP \punkte{15}}

The original loop in \texttt{recur\_seq.c} computes a geometric progression using a loop-carried dependency: each iteration updates the variable \texttt{Sn} by multiplying it with a constant factor \texttt{up}. This dependency prevents direct parallelization. To enable parallel execution, the loop was transformed using the closed-form expression of a geometric sequence: \texttt{opt[n] = pow(up, n)}. This approach eliminates inter-iteration dependencies and allows each iteration to be computed independently.

The parallel implementation in \texttt{recur\_omp.c} uses \texttt{\#pragma omp parallel for} with the \texttt{firstprivate(up)} clause to ensure each thread receives a copy of the multiplier, and \texttt{lastprivate(Sn)} to retain the final value of \texttt{Sn} after the loop. The transformation guarantees correctness regardless of the OpenMP scheduling strategy. The serial version retains the original loop structure to avoid the overhead of the \texttt{pow()} function, which is computationally expensive.

\subsection{Correctness Verification}

The correctness of the parallel implementation was verified by comparing the final value of \texttt{Sn} and the computed norm \texttt{opt\textasciicircum2\_2} across multiple thread counts. The results remained consistent, confirming the mathematical equivalence of the transformed loop.

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{OMP\_NUM\_THREADS} & \textbf{Final Sn} & \textbf{opt\textasciicircum2\_2} \\
\hline
1 & 485165087.9217357 & 5.8846e+15 \\
2 & 485165087.9217357 & 5.8846e+15 \\
4 & 485165087.9217357 & 5.8846e+15 \\
8 & 485165087.9217357 & 5.8846e+15 \\
16 & 485165087.9217357 & 5.8846e+15 \\
\hline
\end{tabular}
\caption{Correctness verification across thread counts}
\end{table}

\subsection{Performance and Speedup Analysis}

The sequential implementation completed in 6.72 seconds. The parallel version was executed with thread counts ranging from 1 to 16. The runtime decreased significantly with increasing threads, demonstrating strong scaling behavior.

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{OMP\_NUM\_THREADS} & \textbf{Parallel Time (s)} & \textbf{Speedup} \\
\hline
1 & 120.74 & 0.056 \\
2 & 57.25 & 0.117 \\
4 & 28.62 & 0.235 \\
8 & 14.34 & 0.468 \\
16 & 7.19 & 0.934 \\
\hline
\end{tabular}
\caption{Speedup of parallel implementation compared to sequential baseline}
\end{table}

\begin{figure}[h!]
\centering
\includegraphics[width=0.6\textwidth]{./figures/Figure 9: speedup_plot.png}
\caption{Speedup vs. Thread Count}
\end{figure}

The parallel implementation becomes beneficial at higher thread counts, approaching the performance of the serial version at 16 threads. The reduced speedup at lower thread counts is attributed to overhead from the \texttt{pow()} function and thread management. The results confirm that the parallelized loop is efficient, scalable, and independent of scheduling strategy.

\end{document}